---
title: "Benchmarking Spatial-Aware Classification Models for Urothelial Cancer Subtypes "
output: html_notebook
---


```{r}
library(Cardinal)
library(ggplot2)
```

# Data import 

```{r load-analyze-files}
TMA1 <- readImzML("TMA1.imzML")
TMA2 <- readImzML("TMA2.imzML")
```

#plot(TMA1, mz=1000.5, tolerance=0.1)


```{r viz_analyze_files}
TMA1 
TMA2 
```

```{r load-spectra-annotations}
TMA1_annotations <- read.delim("TMA1_annotations.txt", header = TRUE, stringsAsFactors = FALSE)
TMA2_annotations <- read.delim("TMA2_annotations.txt", header = TRUE, stringsAsFactors = FALSE)

```


```{r vis_spectra_annotations}

head(TMA1_annotations)
head(TMA2_annotations)

```


```{r}
library(dplyr)

plot_tissue_info <- function(annotations, plot_type = "diagnosis") {
  # Ensure annotations are ordered correctly
  annotations <- annotations %>% arrange(y, x)
  
  # Create the plot data
  plot_data <- annotations %>%
    select(x, y, diagnosis, histology, invasiveness)
  
  # Determine fill based on plot type
  fill_var <- sym(plot_type)
  fill_label <- capitalize(plot_type)
  
  # Create the plot
  ggplot(plot_data, aes(x = x, y = y, fill = !!fill_var)) +
    geom_tile() +
    scale_fill_viridis_d() +
    theme_minimal() +
    labs(title = paste("Tissue", fill_label),
         x = "X coordinate",
         y = "Y coordinate",
         fill = fill_label) +
    coord_fixed(ratio = 1) +  # This ensures squares are square
    theme(legend.position = "right")
}

# Helper function to capitalize first letter
capitalize <- function(x) {
  substr(x, 1, 1) <- toupper(substr(x, 1, 1))
  x
}

# Plot for TMA1
plot_tissue_info(TMA1_annotations, "diagnosis")
plot_tissue_info(TMA1_annotations, "histology")
plot_tissue_info(TMA1_annotations, "invasiveness")

# Plot for TMA2
plot_tissue_info(TMA2_annotations, "diagnosis")
plot_tissue_info(TMA2_annotations, "histology")
plot_tissue_info(TMA2_annotations, "invasiveness")

```
 

a. Generate single m/z images:
```{r}

# For TMA1
plot(TMA2, mz=1000.5, tolerance=0.1)

# For TMA2
#plot(TMA2, mz=1000.5, tolerance=0.1)

```


```{r}
# Plot spectra from specific pixels
plot(TMA1, coord = list(c(75, 60), c(100, 80)))
```
```{r filter_annotations}

# First load and order the annotations
TMA1_annot_ordered <- TMA1_annotations[with(TMA1_annotations, order(y, x)), ]
TMA2_annot_ordered <- TMA2_annotations[with(TMA2_annotations, order(y, x)), ]

# Filter out NA invasiveness values for both TMAs
TMA1_annot_filtered <- TMA1_annot_ordered[!is.na(TMA1_annot_ordered$invasiveness), ]
TMA2_annot_filtered <- TMA2_annot_ordered[!is.na(TMA2_annot_ordered$invasiveness), ]

# Check the dimensions and distributions for both
cat("TMA1 annotations:\n")
cat("Original rows:", nrow(TMA1_annot_ordered), "\n")
cat("Filtered rows:", nrow(TMA1_annot_filtered), "\n")
cat("Invasiveness distribution:\n")
print(table(TMA1_annot_filtered$invasiveness))

cat("\nTMA2 annotations:\n")
cat("Original rows:", nrow(TMA2_annot_ordered), "\n")
cat("Filtered rows:", nrow(TMA2_annot_filtered), "\n")
cat("Invasiveness distribution:\n")
print(table(TMA2_annot_filtered$invasiveness))
```




# Preparing raw and metadata

# 1. First combine metadata with pixel data

```{r create_metadata}
# For TMA1
coord_ROI_TMA1 <- coord(TMA1_ROIs)
coord_string_ROI_1 <- paste(coord_ROI_TMA1$x, coord_ROI_TMA1$y, sep="_")
coord_string_annot_1 <- paste(TMA1_annot_filtered$x, TMA1_annot_filtered$y, sep="_")

# For TMA2
coord_ROI_TMA2 <- coord(TMA2_ROIs)
coord_string_ROI_2 <- paste(coord_ROI_TMA2$x, coord_ROI_TMA2$y, sep="_")
coord_string_annot_2 <- paste(TMA2_annot_filtered$x, TMA2_annot_filtered$y, sep="_")

# Create metadata for both TMAs
TMA1_metadata <- cbind(
    as.data.frame(pixelData(TMA1_ROIs)),
    TMA1_annot_filtered[match(coord_string_ROI_1, coord_string_annot_1), 
                       c("histology", "diagnosis", "invasiveness", "patient")]
)

TMA2_metadata <- cbind(
    as.data.frame(pixelData(TMA2_ROIs)),
    TMA2_annot_filtered[match(coord_string_ROI_2, coord_string_annot_2), 
                       c("histology", "diagnosis", "invasiveness", "patient")]
)

# Verify the results
cat("TMA1 metadata verification:\n")
cat("Number of rows:", nrow(TMA1_metadata), "\n")
cat("Matches ROI count:", nrow(TMA1_metadata) == length(TMA1_ROIs), "\n")

cat("\nTMA2 metadata verification:\n")
cat("Number of rows:", nrow(TMA2_metadata), "\n")
cat("Matches ROI count:", nrow(TMA2_metadata) == length(TMA2_ROIs), "\n")
```

let's create the PositionDataFrames for both TMAs:
```{r create_position_dataframes}
# Create PositionDataFrame for TMA1
pd <- PositionDataFrame(
    coord = TMA1_metadata[, c("x", "y")],
    run = TMA1_metadata$run,
    histology = TMA1_metadata$histology,
    diagnosis = TMA1_metadata$diagnosis,
    invasiveness = TMA1_metadata$invasiveness,
    patient = TMA1_metadata$patient
)

# Create PositionDataFrame for TMA2
pd2 <- PositionDataFrame(
    coord = TMA2_metadata[, c("x", "y")],
    run = TMA2_metadata$run,
    histology = TMA2_metadata$histology,
    diagnosis = TMA2_metadata$diagnosis,
    invasiveness = TMA2_metadata$invasiveness,
    patient = TMA2_metadata$patient
)

# Verify the PositionDataFrames
cat("TMA1 PositionDataFrame structure:\n")
str(pd)

cat("\nTMA2 PositionDataFrame structure:\n")
str(pd2)

# Check class distributions in PositionDataFrames
cat("\nTMA1 invasiveness distribution:\n")
print(table(pd$invasiveness))

cat("\nTMA2 invasiveness distribution:\n")
print(table(pd2$invasiveness))
```

## The PositionDataFrames are correctly created with all metadata and proper distributions. Now let's proceed with attaching these PositionDataFrames to our MSI data and prepare for classification:
```{r attach_position_data}
# Attach PositionDataFrames to MSI data
TMA1_ROIs_final <- TMA1_ROIs
pixelData(TMA1_ROIs_final) <- pd

TMA2_ROIs_final <- TMA2_ROIs
pixelData(TMA2_ROIs_final) <- pd2

# Verify the attachments
cat("TMA1 data verification:\n")
cat("Number of spectra:", length(TMA1_ROIs_final), "\n")
cat("Invasiveness distribution:\n")
print(table(pixelData(TMA1_ROIs_final)$invasiveness))

cat("\nTMA2 data verification:\n")
cat("Number of spectra:", length(TMA2_ROIs_final), "\n")
cat("Invasiveness distribution:\n")
print(table(pixelData(TMA2_ROIs_final)$invasiveness))

# Save the processed data objects (optional)
save(TMA1_ROIs_final, file="TMA1_processed.RData")
save(TMA2_ROIs_final, file="TMA2_processed.RData")
```


```{r preprocessing}
# 1. Bin the data to unit m/z resolution
cat("Binning spectra...\n")
TMA1_binned <- bin(TMA1_ROIs_final, resolution=1, units="mz")
TMA2_binned <- bin(TMA2_ROIs_final, resolution=1, units="mz")

# 2. Normalize the binned data using TIC (Total Ion Current) normalization
cat("Normalizing spectra...\n")
TMA1_normalized <- normalize(TMA1_binned, method="tic")
TMA2_normalized <- normalize(TMA2_binned, method="tic")

# Verify the preprocessing results
cat("\nTMA1 preprocessing verification:\n")
cat("Number of spectra:", length(TMA1_normalized), "\n")
cat("Number of features:", length(mz(TMA1_normalized)), "\n")
cat("Invasiveness distribution:\n")
print(table(pixelData(TMA1_normalized)$invasiveness))

cat("\nTMA2 preprocessing verification:\n")
cat("Number of spectra:", length(TMA2_normalized), "\n")
cat("Number of features:", length(mz(TMA2_normalized)), "\n")
cat("Invasiveness distribution:\n")
print(table(pixelData(TMA2_normalized)$invasiveness))

# Optional: Plot mean spectra to verify normalization
plot(TMA1_normalized, main="TMA1 Mean Spectrum After Preprocessing")
plot(TMA2_normalized, main="TMA2 Mean Spectrum After Preprocessing")

# Save preprocessed data (optional)
save(TMA1_normalized, file="TMA1_preprocessed.RData")
save(TMA2_normalized, file="TMA2_preprocessed.RData")
```


Let's set up the cross-validation for our classification. We'll use TMA2 for training (with cross-validation) and TMA1 for independent testing:


```{r setup_cross_validation}
# Get unique patients from TMA2 (training set)
TMA2_patients <- unique(pixelData(TMA2_normalized)$patient)
cat("Number of unique patients in TMA2:", length(TMA2_patients), "\n")

# Create 5 folds with stratification by patient
set.seed(123) # for reproducibility
n_folds <- 5

# Create patient-level folds
folds <- split(sample(TMA2_patients), rep(1:n_folds, length.out=length(TMA2_patients)))

# Create fold assignments dataframe
fold_assignments <- data.frame(
    patient = unlist(folds),
    fold = rep(1:n_folds, sapply(folds, length))
)

# Add fold assignments to pixel data
pixelData(TMA2_normalized)$fold <- factor(
    match(pixelData(TMA2_normalized)$patient, fold_assignments$patient) %% n_folds + 1
)

# Verify fold assignments
cat("\nFold sizes (number of spectra):\n")
print(table(pixelData(TMA2_normalized)$fold))

# Check class distribution within each fold
cat("\nClass distribution in each fold:\n")
for(i in 1:n_folds) {
    cat("\nFold", i, ":\n")
    print(table(pixelData(TMA2_normalized)$invasiveness[pixelData(TMA2_normalized)$fold == i]))
}

# Verify patient assignment
cat("\nPatient distribution across folds:\n")
print(fold_assignments)

# Save fold assignments for reproducibility (optional)
save(fold_assignments, file="cv_folds.RData")
```


Let's proceed with the classification using PLS (Partial Least Squares) through Cardinal's crossValidate function:
```{r classification}
# Set up cross-validation parameters
ncomp_range <- 1:10  # Test different numbers of components

# Run cross-validation
cat("Running cross-validation...\n")
# Keep data in MSImageSet format
cv_results <- crossValidate(
    fit. = PLS,
    x = TMA2_normalized,  # Use the normalized MSImageSet directly
    y = pixelData(TMA2_normalized)$invasiveness,
    ncomp = ncomp_range,
    .method = "class",
    fold = fold_vector,
    verbose = TRUE
)

# Extract and summarize results
results <- data.frame(
    ncomp = ncomp_range,
    Accuracy = numeric(length(ncomp_range)),
    Sensitivity = numeric(length(ncomp_range)),
    Specificity = numeric(length(ncomp_range))
)

# Calculate performance metrics for each number of components
for(i in seq_along(ncomp_range)) {
    fold_metrics <- sapply(cv_results@model$scores, function(x) {
        c(inf_recall = x["infiltrating", "Recall", i],
          noninf_recall = x["non-infiltrating", "Recall", i])
    })
    
    results$Sensitivity[i] <- mean(fold_metrics["inf_recall",], na.rm = TRUE)
    results$Specificity[i] <- mean(fold_metrics["noninf_recall",], na.rm = TRUE)
    results$Accuracy[i] <- mean(c(results$Sensitivity[i], results$Specificity[i]))
}

# Plot results
par(mfrow = c(2,1))

# Accuracy plot
plot(results$ncomp, results$Accuracy,
     type = "b", col = "black",
     xlab = "Number of Components",
     ylab = "Balanced Accuracy",
     main = "Cross-validation Results",
     ylim = c(0,1))
grid()

# Sensitivity/Specificity plot
plot(results$ncomp, results$Sensitivity,
     type = "b", col = "blue",
     xlab = "Number of Components",
     ylab = "Rate",
     main = "Sensitivity/Specificity by Components",
     ylim = c(0,1))
lines(results$ncomp, results$Specificity,
      type = "b", col = "red")
legend("topright", 
       legend = c("Sensitivity (Infiltrating)", "Specificity (Non-infiltrating)"),
       col = c("blue", "red"),
       lty = 1,
       pch = 1)
grid()

# Print results table
cat("\nCross-validation results:\n")
print(round(results, 3))

# Find optimal number of components
best_ncomp <- which.max(results$Accuracy)
cat("\nOptimal number of components:", best_ncomp, "\n")
cat("Best balanced accuracy:", round(results$Accuracy[best_ncomp], 3), "\n")
```




```{r improve_accuracy}
# Hyperparameter tuning: Try a wider range of components
ncomp_range_extended <- 1:20  # Extend the range

# Run cross-validation with extended range
cat("Running extended cross-validation...\n")
cv_results_extended <- crossValidate(
    fit. = PLS,
    x = TMA2_normalized,
    y = pixelData(TMA2_normalized)$invasiveness,
    ncomp = ncomp_range_extended,
    .method = "class",
    fold = fold_vector,
    verbose = TRUE
)

# Extract and summarize results for extended range
results_extended <- data.frame(
    ncomp = ncomp_range_extended,
    Accuracy = numeric(length(ncomp_range_extended)),
    Sensitivity = numeric(length(ncomp_range_extended)),
    Specificity = numeric(length(ncomp_range_extended))
)

# Calculate performance metrics for each number of components
for(i in seq_along(ncomp_range_extended)) {
    fold_metrics <- sapply(cv_results_extended@model$scores, function(x) {
        c(inf_recall = x["infiltrating", "Recall", i],
          noninf_recall = x["non-infiltrating", "Recall", i])
    })
    
    results_extended$Sensitivity[i] <- mean(fold_metrics["inf_recall",], na.rm = TRUE)
    results_extended$Specificity[i] <- mean(fold_metrics["noninf_recall",], na.rm = TRUE)
    results_extended$Accuracy[i] <- mean(c(results_extended$Sensitivity[i], results_extended$Specificity[i]))
}

# Plot extended results
par(mfrow = c(2,1))

# Accuracy plot
plot(results_extended$ncomp, results_extended$Accuracy,
     type = "b", col = "black",
     xlab = "Number of Components",
     ylab = "Balanced Accuracy",
     main = "Extended Cross-validation Results",
     ylim = c(0,1))
grid()

# Sensitivity/Specificity plot
plot(results_extended$ncomp, results_extended$Sensitivity,
     type = "b", col = "blue",
     xlab = "Number of Components",
     ylab = "Rate",
     main = "Sensitivity/Specificity by Components (Extended)",
     ylim = c(0,1))
lines(results_extended$ncomp, results_extended$Specificity,
      type = "b", col = "red")
legend("topright", 
       legend = c("Sensitivity (Infiltrating)", "Specificity (Non-infiltrating)"),
       col = c("blue", "red"),
       lty = 1,
       pch = 1)
grid()

# Print extended results table
cat("\nExtended cross-validation results:\n")
print(round(results_extended, 3))

# Find optimal number of components in extended range
best_ncomp_extended <- which.max(results_extended$Accuracy)
cat("\nOptimal number of components (extended):", best_ncomp_extended, "\n")
cat("Best balanced accuracy (extended):", round(results_extended$Accuracy[best_ncomp_extended], 3), "\n")
```

# evaluate the final model (using 17 components) on TMA1 and create comprehensive visualizations:

```{r final_model_evaluation}
# Train final model with optimal parameters
final_model <- PLS(
    TMA2_normalized,
    y = pixelData(TMA2_normalized)$invasiveness,
    ncomp = best_ncomp_extended,  # Using 17 components
    .method = "class"
)

# Predict on TMA1 (independent test set)
predictions <- predict(final_model, 
                      newx = TMA1_normalized,
                      .method = "class")

# Check dimensions
cat("\nDimension Check:\n")
cat("Number of predictions:", nrow(predictions), "\n")
cat("Number of actual values:", length(pixelData(TMA1_normalized)$invasiveness), "\n")

# Create subsets matching the actual data length
predictions_subset <- predictions[1:length(pixelData(TMA1_normalized)$invasiveness), ]
predictions_labels <- ifelse(predictions_subset[,1] > predictions_subset[,2], 
                           "infiltrating", "non-infiltrating")

# Create confusion matrix
conf_matrix <- table(
    Predicted = predictions_labels,
    Actual = pixelData(TMA1_normalized)$invasiveness
)

# Calculate performance metrics
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
sensitivity <- conf_matrix["infiltrating", "infiltrating"] / sum(conf_matrix[, "infiltrating"])
specificity <- conf_matrix["non-infiltrating", "non-infiltrating"] / sum(conf_matrix[, "non-infiltrating"])
balanced_accuracy <- mean(c(sensitivity, specificity))
precision <- conf_matrix["infiltrating", "infiltrating"] / sum(conf_matrix["infiltrating", ])
f1_score <- 2 * (precision * sensitivity) / (precision + sensitivity)

# Print comprehensive results
cat("\nFinal Model Performance on TMA1 (Independent Test Set):\n")
cat("----------------------------------------------------\n")
cat("Confusion Matrix:\n")
print(conf_matrix)
cat("\nPerformance Metrics:\n")
cat("Overall Accuracy:", round(accuracy, 3), "\n")
cat("Balanced Accuracy:", round(balanced_accuracy, 3), "\n")
cat("Sensitivity (Infiltrating):", round(sensitivity, 3), "\n")
cat("Specificity (Non-infiltrating):", round(specificity, 3), "\n")
cat("Precision:", round(precision, 3), "\n")
cat("F1 Score:", round(f1_score, 3), "\n")

# Visualizations
par(mfrow = c(2,2))


# 1. ROC Curve
library(pROC)
roc_obj <- roc(pixelData(TMA1_normalized)$invasiveness, predictions_subset[,1])
plot(roc_obj, main = "ROC Curve")
text(0.6, 0.4, paste("AUC =", round(auc(roc_obj), 3)))

# 2. Prediction Probabilities Distribution
hist(predictions_subset[,1], breaks=30,
     main="Distribution of Prediction Probabilities",
     xlab="Probability of Infiltrating",
     col=rgb(0,0,1,0.5))
abline(v=0.5, col="red", lty=2)

# 3. Spatial distribution of predictions
plot(coord(TMA1_normalized)$x, coord(TMA1_normalized)$y,
     col = factor(predictions_labels),
     pch = 16, cex = 0.8,
     main = "Spatial Distribution of Predictions",
     xlab = "X coordinate", ylab = "Y coordinate")
legend("topright", legend = levels(factor(predictions_labels)),
       col = 1:2, pch = 16)

# 4. Prediction confidence by class
boxplot(predictions_subset[,1] ~ pixelData(TMA1_normalized)$invasiveness,
        main = "Prediction Confidence by True Class",
        xlab = "True Class",
        ylab = "Probability of Infiltrating")
abline(h=0.5, col="red", lty=2)

# Create detailed results dataframe
results_df <- data.frame(
    x = coord(TMA1_normalized)$x,
    y = coord(TMA1_normalized)$y,
    true_class = pixelData(TMA1_normalized)$invasiveness,
    predicted_class = predictions_labels,
    infiltrating_prob = predictions_subset[,1],
    patient = pixelData(TMA1_normalized)$patient
)


```
```{r}
library(ggplot2)
library(gridExtra)

# 1. Confusion Matrix Plot
conf_matrix <- matrix(c(305, 145, 104, 124), nrow = 2, byrow = TRUE)
rownames(conf_matrix) <- c("infiltrating", "non-infiltrating")
colnames(conf_matrix) <- c("infiltrating", "non-infiltrating")

conf_matrix_df <- as.data.frame(as.table(conf_matrix))
names(conf_matrix_df) <- c("Predicted", "Actual", "Freq")

p1 <- ggplot(conf_matrix_df, aes(x = Actual, y = Predicted, fill = Freq)) +
    geom_tile() +
    geom_text(aes(label = Freq), color = "black", size = 5) +
    scale_fill_gradient(low = "white", high = "#FF9999") +
    theme_minimal() +
    labs(title = "Confusion Matrix: Partial Least Square(PLS)",
         x = "Actual Class",
         y = "Predicted Class")

# 2. Performance Metrics Bar Plot
metrics_df <- data.frame(
    Metric = c("Overall Accuracy", "Balanced Accuracy", "Sensitivity", 
               "Specificity", "Precision", "F1 Score"),
    Value = c(0.633, 0.603, 0.746, 0.461, 0.678, 0.71)
)

p2 <- ggplot(metrics_df, aes(x = reorder(Metric, -Value), y = Value)) +
    geom_bar(stat = "identity", fill = "#FF9999") +
    coord_flip() +
    theme_minimal() +
    labs(title = "Performance Metrics",
         x = "",
         y = "Score") +
    geom_text(aes(label = sprintf("%.3f", Value)), hjust = -0.1)

# 3. ROC Curve
roc_obj <- roc(pixelData(TMA1_normalized)$invasiveness, predictions_subset[,1])
p3 <- ggplot(data.frame(
    Specificity = 1 - roc_obj$specificities,
    Sensitivity = roc_obj$sensitivities
), aes(x = Specificity, y = Sensitivity)) +
    geom_line(size = 1) +
    geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "gray") +
    theme_minimal() +
    annotate("text", x = 0.75, y = 0.25, 
             label = paste("AUC =", round(auc(roc_obj), 3))) +
    labs(title = "ROC Curve")

# Arrange all plots
grid.arrange(p1, p2, p3, ncol = 1)

# Print performance metrics summary
cat("\nFinal Model Performance Summary:\n")
cat("--------------------------------\n")
cat("Overall Accuracy:", round(0.633, 3), "\n")
cat("Balanced Accuracy:", round(0.603, 3), "\n")
cat("Sensitivity (Infiltrating):", round(0.746, 3), "\n")
cat("Specificity (Non-infiltrating):", round(0.461, 3), "\n")
cat("Precision:", round(0.678, 3), "\n")
cat("F1 Score:", round(0.71, 3), "\n")

```



```{r patient_level_analysis}
library(ggplot2)
library(gridExtra)

# Create data frame for visualization
patient_summary <- data.frame(
    Patient = patient_summary$Patient,
    True_Class = patient_summary$True_Class,
    Accuracy = patient_summary$Accuracy,
    N_Samples = patient_summary$N_Samples
)

# 1. Bubble plot: Accuracy vs Sample Size with patient labels
p1 <- ggplot(patient_summary, 
             aes(x = N_Samples, y = Accuracy, 
                 color = True_Class, size = N_Samples)) +
    geom_point(alpha = 0.6) +
    geom_text(aes(label = Patient), vjust = -1, size = 3) +
    theme_minimal() +
    labs(title = "Patient Performance Overview",
         x = "Number of Samples",
         y = "Accuracy",
         size = "Sample Size") +
    scale_color_manual(values = c("infiltrating" = "#FF9999", 
                                 "non-infiltrating" = "#99CCFF")) +
    theme(legend.position = "right")

# 2. Bar plot with sample size indication
p2 <- ggplot(patient_summary, 
             aes(x = reorder(Patient, -Accuracy), y = Accuracy)) +
    geom_bar(aes(fill = True_Class), stat = "identity") +
    geom_text(aes(label = N_Samples), vjust = -0.5, size = 3) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    labs(title = "Patient Accuracy with Sample Sizes",
         x = "Patient",
         y = "Accuracy",
         fill = "True Class") +
    scale_fill_manual(values = c("infiltrating" = "#FF9999", 
                                "non-infiltrating" = "#99CCFF"))

# Arrange plots
grid.arrange(p1, p2, ncol = 1, heights = c(1, 1))

# Print summary statistics grouped by sample size
cat("\nPerformance by Sample Size Groups:\n")
cat("--------------------------------\n")

# Create sample size categories
patient_summary$Size_Group <- cut(patient_summary$N_Samples, 
                                breaks = c(0, 10, 50, 100, Inf),
                                labels = c("Very Small (≤10)", 
                                         "Small (11-50)", 
                                         "Medium (51-100)",
                                         "Large (>100)"))

# Calculate mean accuracy by size group
size_group_summary <- aggregate(Accuracy ~ Size_Group, 
                              data = patient_summary, 
                              FUN = function(x) c(mean = mean(x), 
                                                count = length(x)))

print(data.frame(
    Size_Group = size_group_summary$Size_Group,
    Mean_Accuracy = round(size_group_summary$Accuracy[,1], 3),
    Number_of_Patients = size_group_summary$Accuracy[,2]
))
```

# SSC method
```{r}
# 1. Setup cross-validation with spatial awareness
set.seed(123)
# Get unique patients from TMA2 (training set)
TMA2_patients <- unique(pixelData(TMA2_normalized)$patient)
n_folds <- 5

# Create patient-level folds
folds <- split(sample(TMA2_patients), rep(1:n_folds, length.out=length(TMA2_patients)))

# Create fold assignments
fold_assignments <- data.frame(
    patient = unlist(folds),
    fold = rep(1:n_folds, sapply(folds, length))
)

# Add fold assignments to pixel data
pixelData(TMA2_normalized)$fold <- factor(
    match(pixelData(TMA2_normalized)$patient, fold_assignments$patient) %% n_folds + 1
)

# 2. Train spatial shrunken centroids with cross-validation
# Try different shrinkage thresholds
thresholds <- seq(0, 10, by = 0.5)
cv_results <- list()

for(threshold in thresholds) {
    cat("Testing threshold:", threshold, "\n")
    
    fold_metrics <- matrix(NA, nrow = n_folds, ncol = 3)  # accuracy, sensitivity, specificity
    colnames(fold_metrics) <- c("accuracy", "sensitivity", "specificity")
    
    for(i in 1:n_folds) {
        # Split data into training and validation
        train_idx <- pixelData(TMA2_normalized)$fold != i
        valid_idx <- pixelData(TMA2_normalized)$fold == i
        
        # Train spatial shrunken centroids model
        ssc_model <- spatialShrunkenCentroids(
            x = TMA2_normalized[,train_idx],
            y = pixelData(TMA2_normalized)$invasiveness[train_idx],
            r = 1,  # spatial radius
            s = threshold,  # shrinkage threshold
            weights = "gaussian"
        )
        
        # Make predictions
        predictions <- predict(ssc_model, 
                            newdata = TMA2_normalized[,valid_idx],
                            type = "class")
        
        # Calculate metrics
        conf_matrix <- table(
            Predicted = predictions,
            Actual = pixelData(TMA2_normalized)$invasiveness[valid_idx]
        )
        
        if(all(dim(conf_matrix) == 2)) {
            sensitivity <- conf_matrix["infiltrating", "infiltrating"] / 
                         sum(conf_matrix[, "infiltrating"])
            specificity <- conf_matrix["non-infiltrating", "non-infiltrating"] / 
                         sum(conf_matrix[, "non-infiltrating"])
            accuracy <- mean(c(sensitivity, specificity))
            
            fold_metrics[i,] <- c(accuracy, sensitivity, specificity)
        }
    }
    
    # Store average results
    cv_results[[as.character(threshold)]] <- colMeans(fold_metrics, na.rm = TRUE)
}

# Would you like me to continue with the rest of the implementation (optimal threshold selection, final model training, and visualization)?


```

#Test on TMA1
```{r}
# Train final model with optimal threshold on subset data
final_ssc_model <- spatialShrunkenCentroids(
    x = TMA2_subset,
    y = pixelData(TMA2_subset)$invasiveness,
    r = 1,  # spatial radius
    s = optimal_threshold,
    weights = "gaussian"
)

# Make predictions on test set
test_predictions <- predict(final_ssc_model, 
                          newdata = TMA1_subset,
                          type = "class")

# Calculate final performance metrics
conf_matrix_ssc <- table(
    Predicted = test_predictions,
    Actual = pixelData(TMA1_subset)$invasiveness
)

accuracy_ssc <- sum(diag(conf_matrix_ssc)) / sum(conf_matrix_ssc)
sensitivity_ssc <- conf_matrix_ssc["infiltrating", "infiltrating"] / 
                  sum(conf_matrix_ssc[, "infiltrating"])
specificity_ssc <- conf_matrix_ssc["non-infiltrating", "non-infiltrating"] / 
                  sum(conf_matrix_ssc[, "non-infiltrating"])
balanced_accuracy_ssc <- mean(c(sensitivity_ssc, specificity_ssc))
precision_ssc <- conf_matrix_ssc["infiltrating", "infiltrating"] / 
                sum(conf_matrix_ssc["infiltrating", ])
f1_score_ssc <- 2 * (precision_ssc * sensitivity_ssc) / 
                (precision_ssc + sensitivity_ssc)

# Print initial results
cat("\nConfusion Matrix:\n")
print(conf_matrix_ssc)
cat("\nPerformance Metrics:\n")
cat("Accuracy:", round(accuracy_ssc, 3), "\n")
cat("Balanced Accuracy:", round(balanced_accuracy_ssc, 3), "\n")
cat("Sensitivity:", round(sensitivity_ssc, 3), "\n")
cat("Specificity:", round(specificity_ssc, 3), "\n")
cat("F1 Score:", round(f1_score_ssc, 3), "\n")


```



# Machine Learning Approach: Random Forest
```{r}

# Convert sparse matrix to regular matrix and prepare data
train_data <- as.matrix(spectra(TMA2_normalized))
train_data <- as.data.frame(t(train_data))
train_labels <- pixelData(TMA2_normalized)$invasiveness

test_data <- as.matrix(spectra(TMA1_normalized))
test_data <- as.data.frame(t(test_data))
test_labels <- pixelData(TMA1_normalized)$invasiveness

# Print dimensions for verification
cat("Data Dimensions:\n")
cat("Training data:", dim(train_data), "\n")
cat("Testing data:", dim(test_data), "\n")
cat("Training labels:", length(train_labels), "\n")
cat("Testing labels:", length(test_labels), "\n")

# Train Random Forest model
library(randomForest)
set.seed(123)  # for reproducibility
rf_model <- randomForest(x = train_data, 
                        y = as.factor(train_labels), 
                        ntree = 500,  # number of trees
                        mtry = sqrt(ncol(train_data)),  # features per split
                        importance = TRUE)  # calculate variable importance

# Make predictions
rf_predictions <- predict(rf_model, newdata = test_data)

# Create confusion matrix
conf_matrix_rf <- table(Predicted = rf_predictions, 
                       Actual = test_labels)

# Calculate performance metrics
accuracy_rf <- sum(diag(conf_matrix_rf)) / sum(conf_matrix_rf)
sensitivity_rf <- conf_matrix_rf["infiltrating", "infiltrating"] / sum(conf_matrix_rf[, "infiltrating"])
specificity_rf <- conf_matrix_rf["non-infiltrating", "non-infiltrating"] / sum(conf_matrix_rf[, "non-infiltrating"])
balanced_accuracy_rf <- mean(c(sensitivity_rf, specificity_rf))
precision_rf <- conf_matrix_rf["infiltrating", "infiltrating"] / sum(conf_matrix_rf["infiltrating", ])
f1_score_rf <- 2 * (precision_rf * sensitivity_rf) / (precision_rf + sensitivity_rf)

# Print results
cat("\nRandom Forest Model Performance:\n")
cat("-----------------------------\n")
cat("Confusion Matrix:\n")
print(conf_matrix_rf)
cat("\nPerformance Metrics:\n")
cat("Overall Accuracy:", round(accuracy_rf, 3), "\n")
cat("Balanced Accuracy:", round(balanced_accuracy_rf, 3), "\n")
cat("Sensitivity (Infiltrating):", round(sensitivity_rf, 3), "\n")
cat("Specificity (Non-infiltrating):", round(specificity_rf, 3), "\n")
cat("Precision:", round(precision_rf, 3), "\n")
cat("F1 Score:", round(f1_score_rf, 3), "\n")

# Get variable importance
importance_scores <- importance(rf_model)
top_features <- head(sort(importance_scores[,1], decreasing=TRUE), 10)
cat("\nTop 10 Most Important Features:\n")
print(top_features)

```

```{r}
# Visualize Random Forest Results
library(ggplot2)

# 1. Confusion Matrix Visualization
conf_matrix_df <- as.data.frame(as.table(conf_matrix_rf))
names(conf_matrix_df) <- c("Predicted", "Actual", "Freq")

ggplot(conf_matrix_df, aes(x = Actual, y = Predicted, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq)) +
  scale_fill_gradient(low = "white", high = "steelblue") +
  theme_minimal() +
  labs(title = "Confusion Matrix-Random Forest Model",
       x = "Actual Class",
       y = "Predicted Class")

# 2. Performance Metrics Visualization
metrics_df <- data.frame(
  Metric = c("Overall Accuracy", "Balanced Accuracy", "Sensitivity", "Specificity", "Precision", "F1 Score"),
  Value = c(accuracy_rf, balanced_accuracy_rf, sensitivity_rf, specificity_rf, precision_rf, f1_score_rf)
)

ggplot(metrics_df, aes(x = reorder(Metric, -Value), y = Value)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  theme_minimal() +
  coord_flip() +
  labs(title = "Random Forest Model Performance Metrics",
       x = "Metric",
       y = "Score") +
  geom_text(aes(label = sprintf("%.3f", Value)), hjust = -0.1)

# 3. Feature Importance Plot
# Get m/z values for top features
mz_values <- mz(TMA2_normalized)
importance_df <- data.frame(
  Feature = names(top_features),
  Importance = as.numeric(top_features),
  mz = mz_values[as.numeric(gsub("V", "", names(top_features)))]
)

ggplot(importance_df, aes(x = reorder(sprintf("%.2f", mz), Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  theme_minimal() +
  coord_flip() +
  labs(title = "Top 10 Most Important Features",
       x = "m/z Value",
       y = "Feature Importance Score")

```





```{r}
# Install and load ROSE package
#install.packages("ROSE")
library(ROSE)

# Check original class distribution
cat("Original class distribution:\n")
print(table(train_labels))

# Apply ROSE to balance the dataset
set.seed(123)  # for reproducibility
balanced_data <- ROSE(
    formula = as.factor(train_labels) ~ .,
    data = data.frame(train_data_selected, train_labels = train_labels),
    N = nrow(train_data_selected),
    seed = 123
)$data

# Check new class distribution
cat("\nBalanced class distribution:\n")
print(table(balanced_data$train_labels))

# Train Random Forest with balanced data
balanced_rf <- randomForest(
    x = balanced_data[, -ncol(balanced_data)],
    y = as.factor(balanced_data$train_labels),
    ntree = 1000,
    mtry = 40,
    importance = TRUE
)

# Make predictions
balanced_predictions <- predict(balanced_rf, newdata = test_data_selected)

# Calculate performance metrics
balanced_conf_matrix <- table(
    Predicted = balanced_predictions, 
    Actual = test_labels
)

# Print results
cat("\nBalanced Random Forest Performance:\n")
cat("--------------------------------\n")
cat("Confusion Matrix:\n")
print(balanced_conf_matrix)

# Calculate and print metrics
balanced_accuracy <- sum(diag(balanced_conf_matrix)) / sum(balanced_conf_matrix)
balanced_sensitivity <- balanced_conf_matrix["infiltrating", "infiltrating"] / 
                       sum(balanced_conf_matrix[, "infiltrating"])
balanced_specificity <- balanced_conf_matrix["non-infiltrating", "non-infiltrating"] / 
                       sum(balanced_conf_matrix[, "non-infiltrating"])
balanced_balanced_acc <- mean(c(balanced_sensitivity, balanced_specificity))
balanced_precision <- balanced_conf_matrix["infiltrating", "infiltrating"] / 
                     sum(balanced_conf_matrix["infiltrating", ])
balanced_f1 <- 2 * (balanced_precision * balanced_sensitivity) / 
              (balanced_precision + balanced_sensitivity)

cat("\nPerformance Metrics:\n")
cat("Overall Accuracy:", round(balanced_accuracy, 3), "\n")
cat("Balanced Accuracy:", round(balanced_balanced_acc, 3), "\n")
cat("Sensitivity (Infiltrating):", round(balanced_sensitivity, 3), "\n")
cat("Specificity (Non-infiltrating):", round(balanced_specificity, 3), "\n")
cat("Precision:", round(balanced_precision, 3), "\n")
cat("F1 Score:", round(balanced_f1, 3), "\n")


```

#try a different approach using class weights instead:

```{r}

# Train Random Forest with class weights
weighted_rf <- randomForest(
    x = train_data_selected,
    y = as.factor(train_labels),
    ntree = 1000,
    mtry = 40,
    classwt = c(
        "infiltrating" = 1,
        "non-infiltrating" = 1.5
    ),
    importance = TRUE
)

# Make predictions
weighted_predictions <- predict(weighted_rf, newdata = test_data_selected)

# Calculate performance metrics
weighted_conf_matrix <- table(
    Predicted = weighted_predictions, 
    Actual = test_labels
)

# Print results
cat("\nWeighted Random Forest Performance:\n")
cat("--------------------------------\n")
cat("Confusion Matrix:\n")
print(weighted_conf_matrix)

# Calculate metrics
weighted_accuracy <- sum(diag(weighted_conf_matrix)) / sum(weighted_conf_matrix)
weighted_sensitivity <- weighted_conf_matrix["infiltrating", "infiltrating"] / 
                       sum(weighted_conf_matrix[, "infiltrating"])
weighted_specificity <- weighted_conf_matrix["non-infiltrating", "non-infiltrating"] / 
                       sum(weighted_conf_matrix[, "non-infiltrating"])
weighted_balanced_acc <- mean(c(weighted_sensitivity, weighted_specificity))
weighted_precision <- weighted_conf_matrix["infiltrating", "infiltrating"] / 
                     sum(weighted_conf_matrix["infiltrating", ])
weighted_f1 <- 2 * (weighted_precision * weighted_sensitivity) / 
              (weighted_precision + weighted_sensitivity)

cat("\nPerformance Metrics:\n")
cat("Overall Accuracy:", round(weighted_accuracy, 3), "\n")
cat("Balanced Accuracy:", round(weighted_balanced_acc, 3), "\n")
cat("Sensitivity (Infiltrating):", round(weighted_sensitivity, 3), "\n")
cat("Specificity (Non-infiltrating):", round(weighted_specificity, 3), "\n")
cat("Precision:", round(weighted_precision, 3), "\n")
cat("F1 Score:", round(weighted_f1, 3), "\n")

```



```{r}

# 1. Parameter Tuning
# Try different mtry values and number of trees
mtry_values <- seq(20, 60, by = 10)
ntree_values <- c(500, 750, 1000)

cat("Starting Parameter Tuning...\n")
tuning_results <- data.frame()

for(mtry in mtry_values) {
    for(ntree in ntree_values) {
        cat("Testing mtry =", mtry, "ntree =", ntree, "\n")
        
        rf_tuned <- randomForest(x = train_data, 
                                y = as.factor(train_labels),
                                ntree = ntree,
                                mtry = mtry)
        
        pred <- predict(rf_tuned, newdata = test_data)
        conf_matrix <- table(Predicted = pred, Actual = test_labels)
        
        # Calculate metrics
        accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
        sensitivity <- conf_matrix["infiltrating", "infiltrating"] / sum(conf_matrix[, "infiltrating"])
        specificity <- conf_matrix["non-infiltrating", "non-infiltrating"] / sum(conf_matrix[, "non-infiltrating"])
        balanced_acc <- mean(c(sensitivity, specificity))
        
        tuning_results <- rbind(tuning_results, 
                               data.frame(mtry = mtry, 
                                        ntree = ntree,
                                        accuracy = accuracy,
                                        balanced_accuracy = balanced_acc))
    }
}

# Find best parameters
best_params <- tuning_results[which.max(tuning_results$balanced_accuracy), ]
cat("\nBest Parameters:\n")
print(best_params)

# 2. Feature Selection and Final Model
# Select top 500 features based on importance
important_features <- names(sort(importance_scores[,1], decreasing=TRUE)[1:500])
train_data_selected <- train_data[, important_features]
test_data_selected <- test_data[, important_features]

# Train final model with best parameters and selected features
final_rf <- randomForest(x = train_data_selected, 
                        y = as.factor(train_labels),
                        ntree = best_params$ntree,
                        mtry = best_params$mtry,
                        importance = TRUE)

# Make predictions
final_predictions <- predict(final_rf, newdata = test_data_selected)

# Calculate final performance metrics
final_conf_matrix <- table(Predicted = final_predictions, Actual = test_labels)
final_accuracy <- sum(diag(final_conf_matrix)) / sum(final_conf_matrix)
final_sensitivity <- final_conf_matrix["infiltrating", "infiltrating"] / sum(final_conf_matrix[, "infiltrating"])
final_specificity <- final_conf_matrix["non-infiltrating", "non-infiltrating"] / sum(final_conf_matrix[, "non-infiltrating"])
final_balanced_acc <- mean(c(final_sensitivity, final_specificity))
final_precision <- final_conf_matrix["infiltrating", "infiltrating"] / sum(final_conf_matrix["infiltrating", ])
final_f1 <- 2 * (final_precision * final_sensitivity) / (final_precision + final_sensitivity)

# Print final results
cat("\nFinal Optimized Model Performance:\n")
cat("--------------------------------\n")
cat("Confusion Matrix:\n")
print(final_conf_matrix)
cat("\nPerformance Metrics:\n")
cat("Overall Accuracy:", round(final_accuracy, 3), "\n")
cat("Balanced Accuracy:", round(final_balanced_acc, 3), "\n")
cat("Sensitivity (Infiltrating):", round(final_sensitivity, 3), "\n")
cat("Specificity (Non-infiltrating):", round(final_specificity, 3), "\n")
cat("Precision:", round(final_precision, 3), "\n")
cat("F1 Score:", round(final_f1, 3), "\n")



```


The ROSE balancing approach has significantly changed the model's behavior, but not in a helpful way:
Before ROSE (Original RF)
Overall Accuracy: 0.830
Sensitivity: 0.917
Specificity: 0.699
F1 Score: 0.867

After ROSE
Overall Accuracy: 0.565 (↓)
Sensitivity: 0.298 (↓)
Specificity: 0.970 (↑)
F1 Score: 0.453 (↓)
The ROSE balancing has:
Dramatically reduced sensitivity
Increased specificity
Overall decreased performance


# Xgboost Approach

```{r}
# 1. Data Preparation and Cleaning
# First, find common features between train and test data
common_features <- intersect(colnames(train_data), colnames(test_data))
cat("Number of common features:", length(common_features), "\n")

# Subset both datasets to use only common features
train_data_clean <- train_data[valid_train_idx, common_features]
test_data_clean <- test_data[valid_test_idx, common_features]

# Clean and prepare labels
train_labels_clean <- train_labels[valid_train_idx]
test_labels_clean <- test_labels[valid_test_idx]

# Convert labels to numeric
train_labels_xgb <- as.numeric(factor(train_labels_clean)) - 1
test_labels_xgb <- as.numeric(factor(test_labels_clean)) - 1

# Verify dimensions and check for NAs
cat("\nAfter alignment:")
cat("\nTraining data dimensions:", dim(train_data_clean))
cat("\nTest data dimensions:", dim(test_data_clean))
cat("\nNA values in training labels:", sum(is.na(train_labels_xgb)))
cat("\nNA values in test labels:", sum(is.na(test_labels_xgb)))

# Create DMatrix objects
dtrain <- xgb.DMatrix(data = as.matrix(train_data_clean), label = train_labels_xgb)
dtest <- xgb.DMatrix(data = as.matrix(test_data_clean), label = test_labels_xgb)

# 2. Set XGBoost Parameters
params <- list(
    objective = "binary:logistic",
    eval_metric = "auc",
    eta = 0.1,
    max_depth = 6,
    min_child_weight = 1,
    subsample = 0.8,
    colsample_bytree = 0.8
)

# 3. Train XGBoost Model
set.seed(123)
xgb_model <- xgb.train(
    params = params,
    data = dtrain,
    nrounds = 100,
    watchlist = list(train = dtrain, test = dtest),
    early_stopping_rounds = 10,
    verbose = 1
)

# 4. Make Predictions
xgb_pred_prob <- predict(xgb_model, dtest)
xgb_predictions <- ifelse(xgb_pred_prob > 0.5, "non-infiltrating", "infiltrating")

# 5. Calculate Performance Metrics
conf_matrix_xgb <- table(
    Predicted = xgb_predictions, 
    Actual = test_labels_clean
)

# Calculate metrics
accuracy_xgb <- sum(diag(conf_matrix_xgb)) / sum(conf_matrix_xgb)
sensitivity_xgb <- conf_matrix_xgb["infiltrating", "infiltrating"] / sum(conf_matrix_xgb[, "infiltrating"])
specificity_xgb <- conf_matrix_xgb["non-infiltrating", "non-infiltrating"] / sum(conf_matrix_xgb[, "non-infiltrating"])
balanced_accuracy_xgb <- mean(c(sensitivity_xgb, specificity_xgb))
precision_xgb <- conf_matrix_xgb["infiltrating", "infiltrating"] / sum(conf_matrix_xgb["infiltrating", ])
f1_score_xgb <- 2 * (precision_xgb * sensitivity_xgb) / (precision_xgb + sensitivity_xgb)

# Print results
cat("\nXGBoost Model Performance:\n")
cat("-------------------------\n")
cat("Confusion Matrix:\n")
print(conf_matrix_xgb)
cat("\nPerformance Metrics:\n")
cat("Overall Accuracy:", round(accuracy_xgb, 3), "\n")
cat("Balanced Accuracy:", round(balanced_accuracy_xgb, 3), "\n")
cat("Sensitivity (Infiltrating):", round(sensitivity_xgb, 3), "\n")
cat("Specificity (Non-infiltrating):", round(specificity_xgb, 3), "\n")
cat("Precision:", round(precision_xgb, 3), "\n")
cat("F1 Score:", round(f1_score_xgb, 3), "\n")

# Get feature importance
importance_matrix <- xgb.importance(model = xgb_model)
cat("\nTop 10 Most Important Features:\n")
print(head(importance_matrix, 10))

```


```{r}

library(ggplot2)
library(gridExtra)
library(reshape2)

# Adjust plot margins and text sizes
plot_theme <- theme_minimal() +
    theme(
        plot.title = element_text(size = 12, face = "bold", margin = margin(b = 10)),
        axis.text = element_text(size = 10),
        axis.title = element_text(size = 11),
        plot.margin = margin(20, 20, 20, 20)
    )

# 1. Confusion Matrix Visualization
p1 <- ggplot(conf_matrix_data, aes(x = Actual, y = Predicted, fill = Freq)) +
    geom_tile() +
    geom_text(aes(label = Freq), color = "black", size = 4) +
    scale_fill_gradient(low = "white", high = "#FF9999") +
    plot_theme +
    labs(title = "XGBoost Confusion Matrix",
         x = "Actual Class",
         y = "Predicted Class")

# 2. Performance Metrics Visualization
p2 <- ggplot(metrics_data, aes(x = reorder(Metric, -Value), y = Value)) +
    geom_bar(stat = "identity", fill = "#FF9999") +
    coord_flip() +
    plot_theme +
    labs(title = "XGBoost Performance Metrics",
         x = "",
         y = "Score") +
    geom_text(aes(label = sprintf("%.3f", Value)), hjust = -0.1, size = 3.5) +
    scale_y_continuous(limits = c(0, 1.1))  # Extend y-axis limit to show all labels

# 3. Feature Importance Plot
p3 <- ggplot(importance_data, aes(x = reorder(sprintf("m/z %.2f", mz), Importance), y = Importance)) +
    geom_bar(stat = "identity", fill = "#FF9999") +
    coord_flip() +
    plot_theme +
    labs(title = "Top 10 Important Features",
         x = "m/z Value",
         y = "Importance Score") +
    geom_text(aes(label = sprintf("%.3f", Importance)), hjust = -0.1, size = 3.5) +
    scale_y_continuous(limits = c(0, max(importance_data$Importance) * 1.2))  # Extend y-axis limit

# 4. Training Progress Plot
p4 <- ggplot(train_log_long, aes(x = Iteration, y = value, color = variable)) +
    geom_line(size = 1) +
    plot_theme +
    theme(legend.position = "bottom") +
    labs(title = "XGBoost Training Progress",
         y = "AUC Score") +
    scale_color_manual(values = c("Train_AUC" = "#FF9999", "Test_AUC" = "#66B2FF"),
                      labels = c("Train AUC", "Test AUC"))

# Arrange plots with more space
grid.arrange(
    p1, p2, p3, p4, 
    ncol = 2,
    widths = c(1, 1),
    heights = c(1, 1),
    layout_matrix = rbind(c(1,2), c(3,4)),
    top = "XGBoost Model Analysis"
)

```
















##First we filter the annotation data to remove all spectra that are not part of tumor or stroma tissues.

```{r prepare-spectra-annotations}

# Keeping only spectra annotations with tumor or stroma annotation in each TMA
#TMA1_invasive <- TMA1_annotations[!is.na(TMA1_annotations$invasiveness), ]
#TMA2_invasive <- TMA2_annotations[!is.na(TMA2_annotations$invasiveness), ]

# Verify the class distribution
#cat("TMA1 invasiveness distribution:\n")
#print(table(TMA1_invasive$invasiveness))

#cat("\nTMA2 invasiveness distribution:\n")
#print(table(TMA2_invasive$invasiveness))
```




```{r metadata-filtering}

# Generate logical vector for spectra with invasiveness annotations
# For TMA1
#coord_TMA1 <- coord(TMA1)
#coord_string_TMA1 <- paste(coord_TMA1$x, coord_TMA1$y, sep="_")
#annotation_string_TMA1 <- paste(TMA1_invasive$x, TMA1_invasive$y, sep="_")
#annotated_spectra_TMA1 <- coord_string_TMA1 %in% annotation_string_TMA1

# For TMA2
#coord_TMA2 <- coord(TMA2)
#coord_string_TMA2 <- paste(coord_TMA2$x, coord_TMA2$y, sep="_")
#annotation_string_TMA2 <- paste(TMA2_invasive$x, TMA2_invasive$y, sep="_")
#annotated_spectra_TMA2 <- coord_string_TMA2 %in% annotation_string_TMA2

```


# Filtering to keep only spectra that have a Invasiness annotation
```{r prepare-spectra-annotations}

# Filter spectra
TMA1_ROIs <- subset(TMA1, annotated_spectra_TMA1)
TMA2_ROIs <- subset(TMA2, annotated_spectra_TMA2)


```


```{r prepare-spectra-annotations}

print(TMA1_ROIs)
print(TMA2_ROIs)

```

```{r verify_content}

head(coord(TMA1_ROIs))
head(coord(TMA2_ROIs))

```


```{r Check_pixel_data}
head(pixelData(TMA1_ROIs))
head(pixelData(TMA2_ROIs))
```


```{r extract_intensity_for_data_further_analysis}

   intensities_TMA1 <- spectra(TMA1_ROIs)
   intensities_TMA2 <- spectra(TMA2_ROIs)
```


```{r visualize_mean_spectrum}

plot(TMA1_ROIs)

plot(TMA2_ROIs)
```



Then we attach the metadata to the raw data.

```{r attach-metadata}

# order spectra coordinates to have them in same order in MSI data and metadata
#TMA1_annot_ordered <- TMA1_tumor_stroma[with(TMA1_tumor_stroma, order(y,x)), ]
#TMA2_annot_ordered <- TMA2_tumor_stroma[with(TMA2_tumor_stroma, order(y,x)), ]


# check if coordinates order is the same
#head(TMA1_annot_ordered)
#head(pixelData(TMA1_ROIs))


#head(TMA2_annot_ordered)
#head(pixelData(TMA2_ROIs))
```


```{r}
# combine the additional metadata and the pixel data (pData) of the MSI data
TMA1_metadata <- cbind(as.data.frame(pixelData(TMA1_ROIs)), TMA1_annot_ordered[4:7])
TMA2_metadata <- cbind(as.data.frame(pixelData(TMA2_ROIs)), TMA2_annot_ordered[4:7])

# attach the metadata dataframe to the MSI data, this requires defining the coordinates and run in the PositionDataFrame, which is a special data frame that holds metadata directly attached to the MSI data

# Create PositionDataFrame for TMA1
pd <- PositionDataFrame(
     coord = TMA1_metadata[, c("x", "y")],
     run = TMA1_metadata$run,
     histology = TMA1_metadata$histology,
     diagnosis = TMA1_metadata$diagnosis,
     invasiveness = TMA1_metadata$invasiveness,
     patient = TMA1_metadata$patient
 )
 
# Check the structure of pd
 str(pd)
 
# Create PositionDataFrame for TMA2
pd2 <- PositionDataFrame(
     coord = TMA2_metadata[, c("x", "y")],
     run = TMA2_metadata$run,
     histology = TMA2_metadata$histology,
     diagnosis = TMA2_metadata$diagnosis,
     invasiveness = TMA2_metadata$invasiveness,
     patient = TMA2_metadata$patient
 )
# Check the structure of pd
 str(pd2)

```


```{r visualize_mean_spectrum}

pixelData(TMA1_ROIs)$patient <- TMA1_metadata$patient
pixelData(TMA2_ROIs)$patient <- TMA2_metadata$patient


# TMA1_metadata has the same order as pixelData(TMA1_ROIs)
pixelData(TMA1_ROIs)$histology <- TMA1_metadata$histology
pixelData(TMA2_ROIs)$histology <- TMA2_metadata$histology

# Verify that it's been added
head(pixelData(TMA1_ROIs)$histology)
head(pixelData(TMA2_ROIs)$histology)
```



To get an overview of the annotations, we visualize the spectra annotations of each file. 
```{r}
patients_plot = ggplot(as.data.frame(pixelData(TMA1_ROIs)), aes(x=x, y=y, fill=patient)) +
  geom_tile(height = 1, width=1, show.legend=FALSE) +
  coord_fixed() +
  ggtitle("Different patients") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(text=element_text(family="ArialMT", face="bold", size=12)) +
  scale_discrete_manual(aesthetics = c("colour", "fill"), 
                        values = colorRampPalette(c("hotpink", "plum", "plum4", "violet", "magenta", "magenta4", "mediumorchid3", "mediumorchid4", "purple", "purple4"))(19))

print(patients_plot)

# calculate mean x and mean y position for each patient tissue
coord_labels = aggregate(cbind(x,y) ~ patient, data=as.data.frame(pixelData(TMA1_ROIs)), mean, na.rm=TRUE, na.action="na.pass") 


# histology
# Color palette
col <- colorRampPalette(c("hotpink", "plum", "plum4", "violet", "magenta", "magenta4", "mediumorchid3", "mediumorchid4", "purple", "purple4"))(39)

# Create plot data
plot_data <- data.frame(
  x = pixelData(TMA1_ROIs)$x,
  y = pixelData(TMA1_ROIs)$y,
  histology = pixelData(TMA1_ROIs)$histology,
  run = pixelData(TMA1_ROIs)$run
)


# Histology plot
histology_plot <- ggplot(plot_data, aes(x = x, y = y, fill = histology)) +
  geom_tile() +
  scale_fill_manual(values = col[c(1, 20)]) +
  coord_fixed() +
  theme_minimal() +
  labs(title = "Tumor and stroma spectra")

print(histology_plot)

# Run plot
run_plot <- ggplot(plot_data, aes(x = x, y = y, fill = run)) +
  geom_tile() +
  scale_fill_manual(values = c("grey48", "grey30")) +
  coord_fixed() +
  theme_minimal() +
  labs(title = "TMA (Run)")

print(run_plot)
```





```{r Patient2_histology_run}


library(ggplot2)
library(dplyr)

# Histology plot using ggplot2
histology_data <- as.data.frame(pData(TMA2_ROIs)) %>%
  select(x, y, histology)

histology_plot <- ggplot(histology_data, aes(x = x, y = y, fill = histology)) +
  geom_tile() +
  scale_fill_manual(values = c("royalblue", "coral2")) +
  coord_fixed() +
  theme_minimal() +
  labs(title = "Tumor and stroma spectra") +
  theme(plot.title = element_text(hjust = 0.5),
        text = element_text(family="ArialMT", face="bold", size=12))

print(histology_plot)

# Run plot using ggplot2
run_data <- as.data.frame(pData(TMA2_ROIs)) %>%
  select(x, y, run)

run_plot <- ggplot(run_data, aes(x = x, y = y, fill = run)) +
  geom_tile() +
  scale_fill_manual(values = c("grey48", "grey30")) +
  coord_fixed() +
  theme_minimal() +
  labs(title = "TMA (Run)") +
  theme(plot.title = element_text(hjust = 0.5),
        text = element_text(family="ArialMT", face="bold", size=12))

print(run_plot)
                       
```


# Pre-processing
For classification, we need to split the data into a training and test data set. 
It is important that the same subjects (patients) are either present in the training or in the test group but not in both. 
For this dataset it means that tumor and stroma region of the same patient have to be in the same set. Ideally, the training and test dataset are independent. 
Even though the tissues in TMA1 and TMA2 were similarly handled and are from a single institute, they were at least measured in separated runs. 
Thus, we will use TMA1 and TMA2 as test and training set respectively in the classification. 
To borrow as little information as possible between the two datasets, we keep them separate during preprocessing and only transfer the m/z positions from TMA2 to TMA1. 

![Overview of the analysis worklow](SSC_WF_case_study3.png)

## Spectral processing and mass alignment

Pre-processing is performed in several steps in order to be able to visualize the intermediate results. 
First we perform spectra smoothing and baseline reduction, which is recommended for low mass resolution MALDI-TOF imaging data.

```{r pp-smoothing-baseline}

# library(Cardinal)
# library(matter)

# # Function to safely get intensity values
# safe_intensity <- function(data) {
#   tryCatch({
#     int <- intensity(data)
#     if (is(int, "matter_list")) {
#       cat("Intensity is a matter_list\n")
#       return(int)
#     } else if (is.matrix(int)) {
#       cat("Intensity is a matrix\n")
#       return(int)
#     } else {
#       cat("Unexpected intensity structure:", class(int), "\n")
#       return(int)
#     }
#   }, error = function(e) {
#     stop(paste("Error getting intensity:", e$message))
#   })
# }

# # Function to preprocess data (smooth and remove baseline)
# preprocess_data <- function(data) {
#   cat("Starting preprocessing...\n")
  
#   # Check data structure
#   cat("Data class:", class(data), "\n")
#   cat("Data dimensions:", dim(data), "\n")
  
#   # Smooth the data
#   cat("Smoothing data...\n")
#   smoothed_data <- smooth(data, method="gaussian", width=8, sd=2)
  
#   # Get intensity values
#   cat("Extracting intensity values...\n")
#   int_values <- safe_intensity(smoothed_data)
#   cat("Intensity class:", class(int_values), "\n")
  
#   if (is(int_values, "matter_list")) {
#     cat("Processing matter_list...\n")
#     # Process each spectrum in the matter_list
#     for (i in seq_along(int_values)) {
#       if (i %% 100 == 0) cat(sprintf("\rProcessing spectrum %d of %d", i, length(int_values)))
#       spectrum <- as.numeric(int_values[[i]])
#       baseline <- median(spectrum, na.rm = TRUE)
#       int_values[[i]] <- spectrum - baseline
#     }
#   } else if (is.matrix(int_values)) {
#     cat("Processing matrix...\n")
#     # Process the matrix as before
#     for (i in seq_len(nrow(int_values))) {
#       if (i %% 100 == 0) cat(sprintf("\rProcessing spectrum %d of %d", i, nrow(int_values)))
#       spectrum <- int_values[i,]
#       baseline <- median(spectrum, na.rm = TRUE)
#       int_values[i,] <- spectrum - baseline
#     }
#   } else {
#     stop("Unsupported intensity data structure")
#   }
  
#   cat("\nBaseline reduction complete.\n")
  
#   # Update the intensity values in the original object
#   cat("Updating intensity values...\n")
#   intensity(smoothed_data) <- int_values
  
#   cat("Preprocessing complete.\n")
#   return(smoothed_data)
# }

# # For TMA1
# cat("Processing TMA1\n")
# TMA1_smoothed_blremoved <- preprocess_data(TMA1_ROIs)

# # Get the coordinates of the first spectrum for TMA1
# first_coord_TMA1 <- coord(TMA1_ROIs)[1, ]

# # Plot before preprocessing for TMA1
# plot(TMA1_ROIs, coord=c(first_coord_TMA1$x, first_coord_TMA1$y), main="TMA1 Before Preprocessing")

# # Plot after preprocessing for TMA1
# plot(TMA1_smoothed_blremoved, coord=c(first_coord_TMA1$x, first_coord_TMA1$y), main="TMA1 After Preprocessing")




# # For TMA2
# cat("Processing TMA2\n")
# TMA2_smoothed_blremoved <- preprocess_data(TMA2_ROIs)

# # Get the coordinates of the first spectrum for TMA2
# first_coord_TMA2 <- coord(TMA2_ROIs)[1, ]

# # Plot before preprocessing for TMA2
# plot(TMA2_ROIs, coord=c(first_coord_TMA2$x, first_coord_TMA2$y), main="TMA2 Before Preprocessing")

# # Plot after preprocessing for TMA2
# plot(TMA2_smoothed_blremoved, coord=c(first_coord_TMA2$x, first_coord_TMA2$y), main="TMA2 After Preprocessing")

# # Save the preprocessed data
# #save(TMA2_smoothed_blremoved, file="TMA2_preprocessed.RData")


# # Save the preprocessed data
# #save(TMA1_smoothed_blremoved, file="TMA1_preprocessed.RData")

# # Clean up to free memory
# #rm(TMA1_ROIs)
# #gc(full=TRUE)
```

Next, we perform m/z alignment in order to remove m/z shifts between spectra. 
# workin New approach
#
```{r}
# library(Cardinal)
# library(magrittr)  # For the %>% operator
# library(MALDIquant)

# # Function to perform peak alignment
# perform_peak_alignment <- function(data, tolerance = 200, units = "ppm") {
#   cat("Performing peak alignment...\n")
  
#   # Calculate mean spectrum
#   cat("Calculating mean spectrum...\n")
#   data_with_mean <- summarizeFeatures(data, stat = "mean")
#   mean_spectrum <- featureData(data_with_mean)$mean
  
#   # Find peaks in the mean spectrum
#   cat("Finding peaks...\n")
#   peaks <- findpeaks(mean_spectrum, SNR = 3)
  
#   # Perform peak alignment
#   cat("Aligning peaks...\n")
#   aligned_data <- peakAlign(data, ref = peaks, tolerance = tolerance, units = units)
  
#   cat("Peak alignment complete.\n")
#   return(aligned_data)
# }

# # Peak alignment for TMA1
# cat("Performing peak alignment for TMA1\n")
# TMA1_aligned <- perform_peak_alignment(TMA1_smoothed_blremoved)

# # Peak alignment for TMA2
# cat("Performing peak alignment for TMA2\n")
# TMA2_aligned <- perform_peak_alignment(TMA2_smoothed_blremoved)

# # Save the aligned data
# #save(TMA1_aligned, file="TMA1_aligned.RData")
# #save(TMA2_aligned, file="TMA2_aligned.RData")

# # Plot to visualize the effect of peak alignment
# par(mfrow=c(2,2))
# plot(TMA1_smoothed_blremoved, mz=1000.5, tolerance=0.5, main="TMA1 Before Alignment")
# plot(TMA1_aligned, mz=1000.5, tolerance=0.5, main="TMA1 After Alignment")
# plot(TMA2_smoothed_blremoved, mz=1000.5, tolerance=0.5, main="TMA2 Before Alignment")
# plot(TMA2_aligned, mz=1000.5, tolerance=0.5, main="TMA2 After Alignment")
```
## Then we perform mass re-calibration by using the same function with internal calibrants as a reference. The internal calibrants angiotensin (m/z 1296.69), substance P (m/z 1347.72), fibrinopeptide B (m/z 1570.68) were mixed with the matrix and thus equally distributed over the tissue. 805.42 is an autolysis peptide of trypsin that we include as well as trypsin was sprayed equally over the tissue section.

# Works
```{r pp-recalibration}

# # Modified calibration approach
# calibrant_mz <- c(805.42, 1296.69, 1347.72, 1570.68)

# # Function to perform recalibration
# perform_recalibration <- function(data, ref_mz, tolerance = 200, units = "ppm") {
#     cat("Performing recalibration...\n")
    
#     # Perform recalibration using the reference m/z values
#     recalibrated_data <- recalibrate(data, 
#                                     ref = ref_mz, 
#                                     tolerance = tolerance, 
#                                     units = units,
#                                     method = "locmax") %>%
#         process()
    
#     cat("Recalibration complete.\n")
#     return(recalibrated_data)
# }

# # Apply recalibration to TMA1 and TMA2
# cat("Recalibrating TMA1\n")
# TMA1_recalibrated <- perform_recalibration(TMA1_aligned, calibrant_mz)

# cat("Recalibrating TMA2\n")
# TMA2_recalibrated <- perform_recalibration(TMA2_aligned, calibrant_mz)

# # Plot to visualize the effect of recalibration
# par(mfrow=c(2,2))
# plot(TMA1_aligned, mz=805.42, tolerance=0.5, main="TMA1 Before Recalibration")
# plot(TMA1_recalibrated, mz=805.42, tolerance=0.5, main="TMA1 After Recalibration")
# plot(TMA2_aligned, mz=805.42, tolerance=0.5, main="TMA2 Before Recalibration")
# plot(TMA2_recalibrated, mz=805.42, tolerance=0.5, main="TMA2 After Recalibration")

```

## Peak picking and filtering

Next, we perform peak picking, alignment and filtering only on TMA2. 
```{r}

# TMA2_recalibrated@centroided <- FALSE
# # Perform peak picking

# # Peak picking, alignment, and filtering for TMA2
# TMA2_pprocessed <- TMA2_recalibrated %>%
#   peakPick(method = "mad", SNR = 5) %>% 
#   peakAlign(tolerance = 200, units = "ppm") %>%
#   peakFilter(freq.min = 0.01) %>%
#   process(blocks = 500)

# # Plot results for TMA2
# plot(TMA2_pprocessed, mz=1000.5, tolerance=0.1, main="TMA2 Processed")



```

# New approach 
```{r pp-peakbinning}

# # Fixed binning function with stricter matching
# bin_peaks <- function(data, reference_mz, tolerance = 200, units = "ppm") {
#     # Convert tolerance to absolute units if needed
#     if(units == "ppm") {
#         abs_tolerance <- reference_mz * (tolerance / 1e6)
#     } else {
#         abs_tolerance <- tolerance
#     }
    
#     # Initialize results matrix
#     matched_indices <- integer()
    
#     # For each reference m/z, find ALL peaks within tolerance
#     for(i in seq_along(reference_mz)) {
#         diffs <- abs(mz(data) - reference_mz[i])
#         within_tolerance <- which(diffs <= abs_tolerance[i])
        
#         # If multiple peaks within tolerance, take the closest one
#         if(length(within_tolerance) > 0) {
#             closest_idx <- within_tolerance[which.min(diffs[within_tolerance])]
#             matched_indices <- c(matched_indices, closest_idx)
#         }
#     }
    
#     # Remove duplicates and sort
#     matched_indices <- sort(unique(matched_indices))
    
#     # Subset the data using only the matched indices
#     if(length(matched_indices) > 0) {
#         result <- data[, matched_indices]
#     } else {
#         warning("No matches found within tolerance")
#         result <- data[, integer(0)]
#     }
    
#     return(result)
# }

# # Apply improved binning to TMA1 and TMA2
# TMA1_peakbinned <- tryCatch({
#     binned <- bin_peaks(TMA1_recalibrated, peakpicked_mz) %>% process()
#     cat("TMA1 binning successful\n")
#     binned
# }, error = function(e) {
#     message("Error processing TMA1: ", e$message)
#     NULL
# })

# TMA2_peakbinned <- tryCatch({
#     binned <- bin_peaks(TMA2_recalibrated, peakpicked_mz) %>% process()
#     cat("TMA2 binning successful\n")
#     binned
# }, error = function(e) {
#     message("Error processing TMA2: ", e$message)
#     NULL
# })

# # Print detailed summary
# if (!is.null(TMA1_peakbinned) && !is.null(TMA2_peakbinned)) {
#     cat("\nSummary:\n")
#     cat("Number of features in original peak list:", length(peakpicked_mz), "\n")
#     cat("Number of features in TMA1 after binning:", length(mz(TMA1_peakbinned)), "\n")
#     cat("Number of features in TMA2 after binning:", length(mz(TMA2_peakbinned)), "\n")
    
#     # Print matched m/z values for verification
#     cat("\nMatched m/z values in TMA1:\n")
#     print(round(mz(TMA1_peakbinned), 2))
#     cat("\nOriginal reference m/z values:\n")
#     print(round(peakpicked_mz, 2))
# }
```


#works 
```{r peakbinned}
# # Define the actual peaks we found in TMA1 that are close to our reference peaks
# TMA1_actual_mz <- c(806, 856, 856, 959, 1308, 1360, 1514, 1566, 1659, 1761, 1812, 2114, 2214)

# # list of picked m/z features
# peakpicked_mz <- mz(TMA2_pprocessed)

# # For TMA1
# TMA1_peakbinned <- TMA1_recalibrated %>%
#   subsetFeatures(mz = TMA1_actual_mz, tolerance = 200, units = "ppm") %>%
#   process()

# # For TMA2
# TMA2_peakbinned <- TMA2_recalibrated %>%
#   subsetFeatures(mz = peakpicked_mz, tolerance = 200, units = "ppm") %>%
#   process()


```



# Data preprocessing
```{r preprocessing}
# Bin the data to unit m/z resolution
TMA1_binned <- bin(TMA1_ROIs, resolution=1, units="mz")
TMA2_binned <- bin(TMA2_ROIs, resolution=1, units="mz")

# Normalize the binned data
TMA1_normalized <- normalize(TMA1_binned, method="tic")
TMA2_normalized <- normalize(TMA2_binned, method="tic")

# For ML models that expect samples x features, transpose the data
# spectra() returns features x samples, so we transpose
TMA1_features <- t(spectra(TMA1_normalized))
TMA2_features <- t(spectra(TMA2_normalized))

# Verify dimensions
cat("TMA1 dimensions (samples x features):", dim(TMA1_features), "\n")
cat("TMA2 dimensions (samples x features):", dim(TMA2_features), "\n")


save(TMA1_normalized, file="TMA1_preprocessed.RData")
save(TMA2_normalized, file="TMA2_preprocessed.RData")


```


# Cross-validation and classification

In order to find optimal classification parameters cross validation is performed. We will split the dataset into 5 groups (also called folds) with 4 patients each. During cross validation the data from 4 groups are used for classification and then tested on the remaining group. This is repeated in order to have all combinations of groups as training and test data. We will perform classification first with the Spatial Shrunken Centroids (SSC) method and repeat it then with the partial least squares regression (PLS) method. 


```{r generating-folds}

# Inputs: 
head(TMA2_metadata)
unique(TMA2_metadata$patient) # 20 patients

## randomly split into 5 groups
set.seed(1)
folds <- split(unique(TMA2_metadata$patient), sample(20, 5))
fold_dataframe <- data.frame(unlist(folds), c(rep(1, 4), rep(2, 4), rep(3, 4), rep (4,4), rep(5,4)))
colnames(fold_dataframe) <- c("patient", "folds")
rownames(fold_dataframe) <- NULL

# merge dataframe with folds to metadata and attach it to the MSI data which will be used for cross validation
pixelData(TMA2_recalibrated)$folds <- merge(TMA2_metadata, fold_dataframe, by.x = "patient")$folds

```


# SSC method with crossvalidation
This code performs cross-validation for a Spatial Shrunken Centroids model:
- Tests different shrinkage parameters (s = 0.01 to 0.2)
- Evaluates model performance with various metrics
- Calculates weighted accuracy based on class distribution (468 infiltrating vs 320 non-infiltrating samples)
- Computes balanced accuracy to handle class imbalance

The goal is to find the optimal shrinkage parameter for the SSC classifier while accounting for spatial relationships in the tissue data.

```{r}
library(Cardinal)


ssc_cv_bladder <- crossValidate(
    fit. = spatialShrunkenCentroids,
    x = TMA2_recalibrated,
    y = TMA2_recalibrated$invasiveness,
    folds = fold_assignments,
    .r = 3,
    s = c(0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2),  # Very small shrinkage values
    r = 3  # Increased neighborhood size
)

# Create results dataframe
ssc_results <- data.frame(
    s = c(0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2),
    Accuracy = numeric(7),
    Sensitivity = numeric(7),
    Specificity = numeric(7),
    Balanced_Accuracy = numeric(7)
)

# Calculate metrics for each shrinkage parameter
for(i in 1:length(ssc_results$s)) {
    fold_metrics <- sapply(ssc_cv_bladder@model$scores, function(x) {
        c(inf_recall = x["infiltrating", "Recall", i],
          noninf_recall = x["non-infiltrating", "Recall", i])
    })
    
    ssc_results$Sensitivity[i] <- mean(fold_metrics["inf_recall",], na.rm = TRUE)
    ssc_results$Specificity[i] <- mean(fold_metrics["noninf_recall",], na.rm = TRUE)
    ssc_results$Accuracy[i] <- (468 * ssc_results$Sensitivity[i] + 
                               320 * ssc_results$Specificity[i]) / (468 + 320)
    ssc_results$Balanced_Accuracy[i] <- mean(c(ssc_results$Sensitivity[i], 
                                             ssc_results$Specificity[i]))
}

```

```{r}
# Create visualization
par(mfrow = c(2,1))

# Balanced Accuracy plot
plot(ssc_results$s, ssc_results$Balanced_Accuracy,
     type = "b", col = "black",
     xlab = "Shrinkage Parameter (s)",
     ylab = "Balanced Accuracy",
     main = "SSC Results: Balanced Accuracy",
     ylim = c(0,1))
grid()

# Sensitivity/Specificity plot
plot(ssc_results$s, ssc_results$Sensitivity,
     type = "b", col = "blue",
     xlab = "Shrinkage Parameter (s)",
     ylab = "Rate",
     main = "SSC Results: Sensitivity/Specificity",
     ylim = c(0,1))
lines(ssc_results$s, ssc_results$Specificity,
      type = "b", col = "red")
legend("topright", 
       legend = c("Sensitivity (Infiltrating)", "Specificity (Non-infiltrating)"),
       col = c("blue", "red"),
       lty = 1,
       pch = 1)
grid()

# Print results
print("\nSSC Results by Shrinkage Parameter:")
print(round(ssc_results, 3))

# Find optimal parameters using balanced accuracy
best_idx <- which.max(ssc_results$Balanced_Accuracy)
cat("\nBest results:\n")
cat("Shrinkage parameter (s):", ssc_results$s[best_idx], "\n")
cat("Overall Accuracy:", round(ssc_results$Accuracy[best_idx], 3), "\n")
cat("Balanced Accuracy:", round(ssc_results$Balanced_Accuracy[best_idx], 3), "\n")
cat("Sensitivity (Infiltrating):", round(ssc_results$Sensitivity[best_idx], 3), "\n")
cat("Specificity (Non-infiltrating):", round(ssc_results$Specificity[best_idx], 3), "\n")

```


```{r}
# Table visualization:

library(kableExtra)
library(dplyr)

# Calculate F1 Score for each row
ssc_results <- ssc_results %>%
  mutate(
    F1_Score = 2 * (Sensitivity * Specificity) / (Sensitivity + Specificity)
  )

# Create formatted table
results_table <- ssc_results %>%
  mutate(
    s = sprintf("%.3f", s),
    Accuracy = sprintf("%.3f", Accuracy),
    Sensitivity = sprintf("%.3f", Sensitivity), 
    Specificity = sprintf("%.3f", Specificity),
    Balanced_Accuracy = sprintf("%.3f", Balanced_Accuracy),
    F1_Score = sprintf("%.3f", F1_Score)
  ) %>%
  rename(
    "Shrinkage (s)" = s,
    "Accuracy" = Accuracy,
    "Sensitivity (Infiltrating)" = Sensitivity,
    "Specificity (Non-infiltrating)" = Specificity,
    "Balanced Accuracy" = Balanced_Accuracy,
    "F1 Score" = F1_Score
  )

# Create styled table
kable(results_table, 
      caption = "SSC Model Performance Metrics",
      align = rep('c', 6)) %>%
  kable_styling(bootstrap_options = c("striped", "hover"),
                full_width = FALSE) %>%
  row_spec(6, background = "#E8F5E9", bold = TRUE) %>%  # Highlight best accuracy row (s=0.150)
  row_spec(5, background = "#E3F2FD", bold = TRUE) %>%  # Highlight best balanced accuracy row (s=0.100)
  column_spec(1, bold = TRUE) %>%
  add_header_above(c(" " = 1, "Performance Metrics" = 5)) %>%
  footnote(
    general = "Best accuracy (green) achieved with s=0.150; Best balanced accuracy (blue) with s=0.100",
    symbol = c("Accuracy considers overall classification performance",
               "Balanced Accuracy represents mean of sensitivity and specificity",
               "F1 Score represents harmonic mean of precision and recall")
  )



```




The PLS method:
Is less sensitive to class imbalance can handle high-dimensional data well often performs better with spectral data
provides information about feature importance

```{r}
# Try PLS classification with cross-validation
pls_cv_bladder <- crossValidate(
    fit. = PLS,  # Change to PLS classifier
    x = TMA2_recalibrated,
    y = TMA2_recalibrated$invasiveness,
    folds = TMA2_recalibrated$folds,
    ncomp = c(2, 3, 4, 5, 6,7,8),  # Number of components to try
    .method = "class",         # For classification
    probability = TRUE,
    trainProcess = process,
    testProcess = process,
    nchunks = getCardinalNChunks(),
    verbose = TRUE,
    BPPARAM = getCardinalBPPARAM()
)

# Analyze PLS results
# First check the structure
cat("PLS model structure:\n")
str(pls_cv_bladder@model$scores[[1]])

# Print the performance metrics
cat("\nPLS average performance:\n")
print(pls_cv_bladder@model$average)


# Extract results for each number of components
ncomp_values <- as.numeric(gsub("ncomp=", "", dimnames(pls_cv_bladder@model$scores[[1]])[[3]]))

# Initialize results dataframe
results <- data.frame(
    ncomp = ncomp_values,
    Accuracy = numeric(length(ncomp_values)),
    Sensitivity = numeric(length(ncomp_values)),  # For infiltrating
    Specificity = numeric(length(ncomp_values))   # For non-infiltrating
)

# Calculate performance metrics
for(i in seq_along(ncomp_values)) {
    fold_metrics <- sapply(pls_cv_bladder@model$scores, function(x) {
        c(inf_recall = x["infiltrating", "Recall", i],
          noninf_recall = x["non-infiltrating", "Recall", i])
    })
    
    results$Sensitivity[i] <- mean(fold_metrics["inf_recall",], na.rm = TRUE)
    results$Specificity[i] <- mean(fold_metrics["noninf_recall",], na.rm = TRUE)
    results$Accuracy[i] <- mean(c(results$Sensitivity[i], results$Specificity[i]))
}

# Plot results
par(mfrow = c(2,1))

# Accuracy plot
plot(results$ncomp, results$Accuracy,
     type = "b", col = "black",
     xlab = "Number of Components",
     ylab = "Accuracy",
     main = "PLS Cross-validation Results: Accuracy",
     ylim = c(0,1))
grid()

# Sensitivity/Specificity plot
plot(results$ncomp, results$Sensitivity,
     type = "b", col = "blue",
     xlab = "Number of Components",
     ylab = "Rate",
     main = "PLS Results: Sensitivity/Specificity",
     ylim = c(0,1))
lines(results$ncomp, results$Specificity,
      type = "b", col = "red")
legend("topright", 
       legend = c("Sensitivity (Infiltrating)", "Specificity (Non-infiltrating)"),
       col = c("blue", "red"),
       lty = 1,
       pch = 1)
grid()

# Print numerical results
cat("\nPLS cross-validation results:\n")
print(results)

# Find optimal parameters
best_idx <- which.max(results$Accuracy)
cat("\nBest results:\n")
cat("Number of components:", results$ncomp[best_idx], "\n")
cat("Accuracy:", round(results$Accuracy[best_idx], 3), "\n")
cat("Sensitivity (Infiltrating):", round(results$Sensitivity[best_idx], 3), "\n")
cat("Specificity (Non-infiltrating):", round(results$Specificity[best_idx], 3), "\n")


```




```{r}

# Create results dataframe with correct number of components
results <- data.frame(
    ncomp = 1:7,  # Changed from 1:10 to 1:7
    Accuracy = numeric(7),
    Sensitivity = numeric(7),
    Specificity = numeric(7)
)

# Calculate metrics for each component
for(i in 1:7) {  # Changed from 1:10 to 1:7
    fold_metrics <- sapply(pls_cv_bladder@model$scores, function(x) {
        c(inf_recall = x["infiltrating", "Recall", i],
          noninf_recall = x["non-infiltrating", "Recall", i])
    })
    
    results$Sensitivity[i] <- mean(fold_metrics["inf_recall",], na.rm = TRUE)
    results$Specificity[i] <- mean(fold_metrics["noninf_recall",], na.rm = TRUE)
    results$Accuracy[i] <- mean(c(results$Sensitivity[i], results$Specificity[i]))
}

# Plot results
par(mfrow = c(2,1))

# Accuracy plot
plot(results$ncomp, results$Accuracy,
     type = "b", col = "black",
     xlab = "Number of Components",
     ylab = "Accuracy",
     main = "PLS Cross-validation Results: Accuracy",
     ylim = c(0,1))
grid()

# Sensitivity/Specificity plot
plot(results$ncomp, results$Sensitivity,
     type = "b", col = "blue",
     xlab = "Number of Components",
     ylab = "Rate",
     main = "PLS Results: Sensitivity/Specificity",
     ylim = c(0,1))
lines(results$ncomp, results$Specificity,
      type = "b", col = "red")
legend("topright", 
       legend = c("Sensitivity (Infiltrating)", "Specificity (Non-infiltrating)"),
       col = c("blue", "red"),
       lty = 1,
       pch = 1)
grid()

# Print results
print(results)

# Find optimal parameters
best_idx <- which.max(results$Accuracy)
cat("\nBest results:\n")
cat("Number of components:", results$ncomp[best_idx], "\n")
cat("Accuracy:", round(results$Accuracy[best_idx], 3), "\n")
cat("Sensitivity:", round(results$Sensitivity[best_idx], 3), "\n")
cat("Specificity:", round(results$Specificity[best_idx], 3), "\n")



```



# SMOTE resampling:
```{r}

# Extract original data
feature_matrix <- t(spectra(TMA2_recalibrated))
response <- TMA2_recalibrated$invasiveness

# Remove NA values
valid_idx <- !is.na(response)
feature_matrix_clean <- feature_matrix[valid_idx, ]
response_clean <- response[valid_idx]

# Create dataframe for SMOTE with explicit factor levels
data_for_smote <- data.frame(
    response = factor(response_clean, 
                     levels = c("infiltrating", "non-infiltrating")),
    feature_matrix_clean
)

# Print initial class distribution in SMOTE data
cat("Class distribution before SMOTE:\n")
print(table(data_for_smote$response))

# Apply SMOTE
set.seed(123)  # For reproducibility
smote_data <- ROSE::ROSE(
    formula = response ~ ., 
    data = data_for_smote,
    N = sum(!is.na(response)),
    p = 0.5,
    seed = 123
)$data

# Create new dataset
TMA2_balanced <- TMA2_recalibrated

# Ensure the invasiveness column is a factor with correct levels
TMA2_balanced$invasiveness <- factor(TMA2_balanced$invasiveness,
                                   levels = c("infiltrating", "non-infiltrating"))

# Replace non-NA values with SMOTE results
TMA2_balanced$invasiveness[valid_idx] <- as.character(smote_data$response)

# Verify results
cat("\nOriginal distribution:\n")
print(table(TMA2_recalibrated$invasiveness, useNA = "ifany"))

cat("\nBalanced distribution:\n")
print(table(TMA2_balanced$invasiveness, useNA = "ifany"))

# Additional verification
cat("\nFactor levels in balanced data:", levels(TMA2_balanced$invasiveness), "\n")
cat("Number of NA values:", sum(is.na(TMA2_balanced$invasiveness)), "\n")
cat("Number of non-NA values:", sum(!is.na(TMA2_balanced$invasiveness)), "\n")


# Run cross-validation on balanced data
pls_cv_bladder_smote <- crossValidate(
    fit. = PLS,
    x = TMA2_balanced,
    y = TMA2_balanced$invasiveness,
    folds = TMA2_balanced$folds,
    ncomp = 1:7,
    process = TRUE,
    .method = "class",
    probability = TRUE,
    trainProcess = process,
    testProcess = process,
    nchunks = getCardinalNChunks(),
    BPPARAM = getCardinalBPPARAM()
)

# Calculate and compare results
results_smote <- data.frame(
    ncomp = 1:7,
    Accuracy = numeric(7),
    Sensitivity = numeric(7),
    Specificity = numeric(7)
)

# Calculate metrics for SMOTE results
for(i in 1:7) {
    fold_metrics <- sapply(pls_cv_bladder_smote@model$scores, function(x) {
        c(inf_recall = x["infiltrating", "Recall", i],
          noninf_recall = x["non-infiltrating", "Recall", i])
    })
    
    results_smote$Sensitivity[i] <- mean(fold_metrics["inf_recall",], na.rm = TRUE)
    results_smote$Specificity[i] <- mean(fold_metrics["noninf_recall",], na.rm = TRUE)
    results_smote$Accuracy[i] <- mean(c(results_smote$Sensitivity[i], results_smote$Specificity[i]))
}

# Print results
print("Results with SMOTE balancing:")
print(results_smote)

# Find optimal parameters
best_idx <- which.max(results_smote$Accuracy)
cat("\nBest results with SMOTE:\n")
cat("Number of components:", results_smote$ncomp[best_idx], "\n")
cat("Accuracy:", round(results_smote$Accuracy[best_idx], 3), "\n")
cat("Sensitivity:", round(results_smote$Sensitivity[best_idx], 3), "\n")
cat("Specificity:", round(results_smote$Specificity[best_idx], 3), "\n")


# Combine results for plotting
results_original$model <- "Original"
results_smote$model <- "SMOTE"
all_results <- rbind(results_original, results_smote)

# Create comparison plots using ggplot2
library(ggplot2)

# 1. Overall Accuracy Comparison
p1 <- ggplot(all_results, aes(x = ncomp, y = Accuracy, color = model)) +
    geom_line() +
    geom_point() +
    theme_minimal() +
    labs(title = "Accuracy Comparison",
         x = "Number of Components",
         y = "Accuracy") +
    theme(legend.position = "bottom")

# 2. Sensitivity/Specificity Comparison
results_long <- tidyr::pivot_longer(all_results, 
                                  cols = c("Sensitivity", "Specificity"),
                                  names_to = "Metric",
                                  values_to = "Value")

p2 <- ggplot(results_long, aes(x = ncomp, y = Value, color = interaction(model, Metric))) +
    geom_line() +
    geom_point() +
    theme_minimal() +
    labs(title = "Sensitivity/Specificity Comparison",
         x = "Number of Components",
         y = "Value",
         color = "Model & Metric") +
    theme(legend.position = "bottom")

# Arrange plots
gridExtra::grid.arrange(p1, p2, ncol = 1)

# Print summary comparison at optimal components
cat("\nComparison at optimal components:\n")
cat("\nOriginal Model (ncomp = 6):\n")
cat("Accuracy:", round(results_original$Accuracy[6], 3), "\n")
cat("Sensitivity:", round(results_original$Sensitivity[6], 3), "\n")
cat("Specificity:", round(results_original$Specificity[6], 3), "\n")

cat("\nSMOTE Model (ncomp = 5):\n")
cat("Accuracy:", round(results_smote$Accuracy[5], 3), "\n")
cat("Sensitivity:", round(results_smote$Sensitivity[5], 3), "\n")
cat("Specificity:", round(results_smote$Specificity[5], 3), "\n")
```


# Compare all approaches including new SMOTE results
```{r}
results_original <- get_model_results(pls_cv_bladder, "Original")
results_weighted <- get_model_results(pls_cv_bladder_weighted, "Weighted")
results_smote <- get_model_results(pls_cv_bladder_smote, "SMOTE-balanced")

# Combine all results
all_results <- rbind(results_original, results_weighted, results_smote)

# Create enhanced comparison plot with updated parameters
ggplot(all_results, aes(x = ncomp, y = Accuracy, color = model)) +
    geom_line(linewidth = 1) +  # Changed from size to linewidth
    geom_point(size = 3) +
    theme_minimal() +
    labs(title = "Comparison of Different Approaches",
         x = "Number of Components",
         y = "Accuracy",
         color = "Model Type") +
    scale_color_manual(values = c("Original" = "red", 
                                "Weighted" = "blue", 
                                "SMOTE-balanced" = "green")) +
    theme(legend.position = "right",
          plot.title = element_text(size = 14, face = "bold"),
          axis.title = element_text(size = 12),
          legend.title = element_text(size = 12),
          legend.text = element_text(size = 10)) +
    scale_y_continuous(limits = c(0.5, 0.65))

# Print numerical comparison of best results for each approach
cat("\nBest results comparison:\n")
cat("\nOriginal model:\n")
best_orig <- which.max(results_original$Accuracy)
cat("Components:", results_original$ncomp[best_orig],
    "\nAccuracy:", round(results_original$Accuracy[best_orig], 3),
    "\nSensitivity:", round(results_original$Sensitivity[best_orig], 3),
    "\nSpecificity:", round(results_original$Specificity[best_orig], 3), "\n")

cat("\nWeighted model:\n")
best_weight <- which.max(results_weighted$Accuracy)
cat("Components:", results_weighted$ncomp[best_weight],
    "\nAccuracy:", round(results_weighted$Accuracy[best_weight], 3),
    "\nSensitivity:", round(results_weighted$Sensitivity[best_weight], 3),
    "\nSpecificity:", round(results_weighted$Specificity[best_weight], 3), "\n")

cat("\nSMOTE-balanced model:\n")
best_smote <- which.max(results_smote$Accuracy)
cat("Components:", results_smote$ncomp[best_smote],
    "\nAccuracy:", round(results_smote$Accuracy[best_smote], 3),
    "\nSensitivity:", round(results_smote$Sensitivity[best_smote], 3),
    "\nSpecificity:", round(results_smote$Specificity[best_smote], 3), "\n")

```



```{r}

# Create a data frame with the best results
best_results <- data.frame(
    Model = c("Original", "Weighted", "SMOTE-balanced"),
    Components = c(7, 6, 5),
    Accuracy = c(0.552, 0.546, 0.621),
    Sensitivity = c(0.736, 0.725, 0.587),
    Specificity = c(0.369, 0.368, 0.656)
)

# Convert to long format for plotting
library(tidyr)
best_results_long <- pivot_longer(best_results, 
                                cols = c("Accuracy", "Sensitivity", "Specificity"),
                                names_to = "Metric",
                                values_to = "Value")

# Create grouped bar plot
ggplot(best_results_long, aes(x = Model, y = Value, fill = Metric)) +
    geom_bar(stat = "identity", position = position_dodge(width = 0.9)) +
    theme_minimal() +
    labs(title = "Comparison of Best Results Across Models",
         x = "Model Type",
         y = "Value",
         fill = "Metric") +
    scale_fill_manual(values = c("Accuracy" = "#2ecc71",
                                "Sensitivity" = "#3498db",
                                "Specificity" = "#e74c3c")) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1),
          plot.title = element_text(size = 14, face = "bold"),
          axis.title = element_text(size = 12),
          legend.title = element_text(size = 12),
          legend.text = element_text(size = 10)) +
    geom_text(aes(label = sprintf("%.3f", Value)), 
              position = position_dodge(width = 0.9),
              vjust = -0.5,
              size = 3.5) +
    ylim(0, max(best_results_long$Value) * 1.1)  # Add some space for labels

# Add a note about components
components_note <- paste("Best number of components:",
                        "Original: 7,",
                        "Weighted: 6,",
                        "SMOTE-balanced: 5")



```


#Predication
The code uses a 5-component PLS model trained on balanced data to predict tumor invasiveness in new tissue samples.
```{r}

 
# 1. First verify our SMOTE-balanced data
print("Distribution in TMA2_balanced:")
print(table(TMA2_balanced$invasiveness, useNA = "ifany"))

# 2. Create PLS model with SMOTE-balanced data
pls_model <- PLS(TMA2_balanced,
                 y = TMA2_balanced$invasiveness,
                 ncomp = 5,  # optimal number from previous analysis
                 .method = "class")

# 3. Verify the model
print("\nModel summary:")
print(pls_model)

# 4. Now let's add the invasiveness data to TMA1_recalibrated
TMA1_new_metadata <- cbind(
    as.data.frame(pixelData(TMA1_recalibrated)),
    TMA1_annot_ordered[4:7]
)

# 5. Create new PositionDataFrame
pd_new <- PositionDataFrame(
    coord = TMA1_new_metadata[, c("x", "y")],
    run = TMA1_new_metadata$run,
    histology = TMA1_new_metadata$histology,
    diagnosis = TMA1_new_metadata$diagnosis,
    invasiveness = TMA1_new_metadata$invasiveness,
    patient = TMA1_new_metadata$patient
)

# 6. Update TMA1_recalibrated with new metadata
pixelData(TMA1_recalibrated) <- pd_new

# 7. Make predictions
predictions <- predict(pls_model,
                      newx = TMA1_processed,
                      ncomp = 5,
                      type = "class")

# 8. Evaluate predictions
actual_classes <- TMA1_recalibrated$invasiveness
predicted_classes <- predictions

# Create valid cases mask
valid_cases <- !is.na(actual_classes) & !is.na(predicted_classes)

# Create confusion matrix
conf_matrix <- table(Predicted = predicted_classes[valid_cases], 
                    Actual = actual_classes[valid_cases],
                    dnn = c("Predicted", "Actual"))

# Print results
print("\nConfusion Matrix:")
print(conf_matrix)

# Calculate metrics
if(sum(conf_matrix) > 0) {
    metrics <- list(
        Accuracy = sum(diag(conf_matrix)) / sum(conf_matrix),
        Sensitivity = conf_matrix["infiltrating", "infiltrating"] / 
                     sum(conf_matrix[, "infiltrating"]),
        Specificity = conf_matrix["non-infiltrating", "non-infiltrating"] / 
                     sum(conf_matrix[, "non-infiltrating"])
    )
    
    print("\nPerformance Metrics on TMA1:")
    for(metric in names(metrics)) {
        cat(metric, ": ", round(metrics[[metric]], 3), "\n")
    }
}



```








